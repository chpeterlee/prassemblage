- Sourced from: https://www.lesswrong.com/tag/inferential-distance

- **Inferential Distance** between two people with respect to an item of knowledge is the amount of steps or concepts a person needs to share before they can successfully communicate the [object level](https://www.lesswrong.com/tag/object-level-and-meta-level) point. This can be thought of as the missing foundation or building block concepts needed to think clearly about a specific thing.

- In [Expecting Short Inferential Distances](https://www.lesswrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances), Eliezer Yudkowsky posits that humans systematically underestimate inferential distances.

- __And if you think you can explain the concept of “systematically underestimated inferential distances” briefly, in just a few words, I’ve got some sad news for you . . . – __[Expecting Short Inferential Distances](https://www.lesswrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances)

- Example: Evidence for Evolution
	 - Explaining the [evidence](https://www.lesswrong.com/tag/evidence) for the theory of [evolution](https://www.lesswrong.com/tag/evolution) to a physicist would be easy; even if the physicist didn't already know about evolution, they would understand the concepts of evidence, [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor), naturalistic explanations, and the general orderly nature of the universe. Explaining the evidence for the theory of evolution to someone without a science background would be much harder. Before even mentioning the specific evidence for evolution, you would have to explain the concept of evidence, why some kinds of evidence are more valuable than others, what does and doesn't count as evidence, and so on. This would be unlikely to work during a short conversation.

	 - There is a short inferential distance between you and the physicist; there is a very long inferential distance between you and the person without any science background. Many members of Less Wrong believe that expecting short inferential distances is a classic error. It is also a very difficult problem to solve, since most people will feel [offended](https://wiki.lesswrong.com/wiki/offence) if you explicitly say that there is too great an inferential distance between you to explain a theory properly. Some people have attempted to explain this through [evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology): in the ancestral environment, there was minimal difference in knowledge between people, and therefore no need to worry about inferential distances.

- External Links
	 - [Why It's Hard to Explain Things: Inferential Distance](http://everydayutilitarian.com/essays/why-its-hard-to-explain-things-inferential-distance/) by Peter Hurford

	 - [How all human communication fails, except by accident, or a commentary of Wiio's laws](https://jkorpela.fi/wiio.html)

- See Also
	 - [General knowledge](https://www.lesswrong.com/tag/general-knowledge)

	 - [Modesty argument](https://www.lesswrong.com/tag/modesty-argument)

	 - [Illusion of transparency](https://www.lesswrong.com/tag/illusion-of-transparency)

	 - [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)

	 - [Common Knowledge](https://www.lesswrong.com/tag/common-knowledge)
